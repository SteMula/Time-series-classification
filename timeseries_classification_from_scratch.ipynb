{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ChzQhCoJv7Nz"
      },
      "source": [
        "# Timeseries classification from scratch\n",
        "\n",
        "**Author:** [SteMula](https://github.com/SteMula/)<br>\n",
        "**Last modified:** 2022/05/8<br>\n",
        "**Description:** Training a timeseries classifier from scratch on the Bearings faul dataset from the Case Western Reserve Universiti archive.\n",
        "https://engineering.case.edu/bearingdatacenter/download-data-file <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTH9KVKXv7N7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqNPXBRqv7N7"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note you have to download the file in local in order to import it as dataset.\n",
        "\n",
        "97.mat is the dataset with the NORMAL SIGNAL\n",
        "\n",
        "https://engineering.case.edu/sites/default/files/97.mat\n",
        "\n",
        "105.mat is the dataset with the INNER RACE FAULT SIGNAL\n",
        "\n",
        "https://engineering.case.edu/sites/default/files/105.mat\n",
        "\n",
        "118.mat is the dataset with the BALL FAULT SIGNAL\n",
        "\n",
        "https://engineering.case.edu/sites/default/files/118.mat\n",
        "\n",
        "130.mat is the dataset with the OUTER RACE CENTERED FAULT SIGNAL\n",
        "\n",
        "https://engineering.case.edu/sites/default/files/130.mat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat = scipy.io.loadmat('97.mat')\n",
        "mat = mat['X097_DE_time']\n",
        "\n",
        "mat_innerrace = scipy.io.loadmat('105.mat')\n",
        "mat_innerrace = mat_innerrace['X105_DE_time']\n",
        "\n",
        "mat_ball = scipy.io.loadmat('118.mat')\n",
        "mat_ball = mat_ball['X118_DE_time']\n",
        "\n",
        "mat_outer = scipy.io.loadmat('130.mat')\n",
        "mat_outer = mat_outer['X130_DE_time']\n",
        "\n",
        "print('normal', len(mat))\n",
        "print('mat_innerrace', len(mat_innerrace))\n",
        "print('mat_ball', len(mat_ball))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following function \"split_data\" I manipulate the input file I got before returning the data for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data(data, len_desired,class_number):\n",
        "    # data is a numpy array of dimension (x,)\n",
        "    # len_ desired is the size in which you want to split it, e.g. 500\n",
        "    # example data is of dimension 243938 and you set len_desired of 500\n",
        "    t_train = 0.67\n",
        "    \n",
        "    data_train = data[:int((len(data)*t_train))]\n",
        "    data_test = data[int((len(data)*t_train)):]\n",
        "\n",
        "    N_train = len(data_train)//len_desired\n",
        "    x_train_ = data_train[:int((N_train*len_desired))]\n",
        "    \n",
        "    x_train_ = np.reshape(x_train_,(N_train,len_desired))\n",
        "    y_train_ = class_number*np.ones(N_train)\n",
        "\n",
        "    N_test = len(data_test)//len_desired\n",
        "\n",
        "    x_test_ = data_test[:int((N_test*len_desired))]\n",
        "\n",
        "    x_test_ = np.reshape(x_test_,(N_test,len_desired))\n",
        "    y_test_ = class_number*np.ones(N_test)\n",
        "\n",
        "\n",
        "    return x_train_, y_train_ , x_test_,y_test_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_mat, y_train_mat, x_test_mat, y_test_mat = split_data(np.squeeze(mat),500,0)\n",
        "x_train_mat_ball, y_train_mat_ball, x_test_mat_ball, y_test_mat_ball = split_data(np.squeeze(mat_ball),500,1)\n",
        "x_train_mat_innerrace, y_train_mat_innerrace, x_test_mat_innerrace, y_test_mat_innerrace= split_data(np.squeeze(mat_innerrace),500,2)\n",
        "x_train_mat_outer, y_train_mat_outer, x_test_mat_outer, y_test_mat_outer= split_data(np.squeeze(mat_outer),500,3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYA21tTCx7vV",
        "outputId": "21b5b133-6c0e-4cca-f539-16efeff92329"
      },
      "outputs": [],
      "source": [
        "x_train = np.concatenate((x_train_mat,x_train_mat_ball,x_train_mat_innerrace,x_train_mat_outer),axis=0)\n",
        "y_train = np.concatenate((y_train_mat,y_train_mat_ball,y_train_mat_innerrace,y_train_mat_outer),axis=0)\n",
        "x_test = np.concatenate((x_test_mat,x_test_mat_ball,x_test_mat_innerrace,x_test_mat_outer),axis=0)\n",
        "y_test = np.concatenate((y_test_mat,y_test_mat_ball,y_test_mat_innerrace,y_test_mat_outer),axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WT6yiSIv7N_"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Here we visualize one timeseries example for each class in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "QTXUcaYyv7N_",
        "outputId": "732ca6f3-bb00-4ecd-e667-20908ba5b880"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
        "print(classes)\n",
        "plt.figure()\n",
        "for c in range(0,2): #replace range(0,1) with classe\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for c in range(2,4):\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ojUtZ0v7OA"
      },
      "source": [
        "## Standardize the data\n",
        "\n",
        "Our timeseries are already in a single length (500). However, their values are\n",
        "usually in various ranges. This is not ideal for a neural network;\n",
        "in general we should seek to make the input values normalized.\n",
        "For this specific dataset, the data is already z-normalized: each timeseries sample\n",
        "has a mean equal to zero and a standard deviation equal to one. This type of\n",
        "normalization is very common for timeseries classification problems, see\n",
        "[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).\n",
        "\n",
        "Note that the timeseries data used here are univariate, meaning we only have one channel\n",
        "per timeseries example.\n",
        "We will therefore transform the timeseries into a multivariate one with one channel\n",
        "using a simple reshaping via numpy.\n",
        "This will allow us to construct a model that is easily applicable to multivariate time\n",
        "series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpcgM05Wv7OB"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvoLQE1v7OB"
      },
      "source": [
        "Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\n",
        "the number of classes beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH5mLdN6v7OB"
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(y_train))\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZND2EWNv7OD"
      },
      "source": [
        "Now we shuffle the training set because we will be using the `validation_split` option\n",
        "later when training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Ip63Cxv7OE"
      },
      "outputs": [],
      "source": [
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2bNvmGyv7OE"
      },
      "source": [
        "Standardize the labels to positive integers.\n",
        "The expected labels will then be 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm0qbslfv7OF"
      },
      "outputs": [],
      "source": [
        "# y_train[y_train == 0] = 0\n",
        "# y_test[y_test == 0] = 0\n",
        "\n",
        "# y_train[y_train == 2] = 1\n",
        "# y_test[y_test == 2] = 1\n",
        "\n",
        "# y_train[y_train == 4 ] = 2\n",
        "# y_test[y_test == 4] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFAnMTjv7OF"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We build a Fully Convolutional Neural Network originally proposed in\n",
        "[this paper](https://arxiv.org/abs/1611.06455).\n",
        "The implementation is based on the TF 2 version provided\n",
        "[here](https://github.com/hfawaz/dl-4-tsc/).\n",
        "The following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found\n",
        "via random search using [KerasTuner](https://github.com/keras-team/keras-tuner)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cnc_cy03v7OF",
        "outputId": "d9cb94e1-44d5-47f4-bed0-459d10a4280b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "model = make_model(input_shape=x_train.shape[1:])\n",
        "keras.utils.plot_model(model, show_shapes=False)\n",
        "\n",
        "\n",
        "# honestly I kept the shape of the net as it was done in the example since it gave me good result.\n",
        "# Probably you can do better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbhqTveTv7OF"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "xN1atr7kv7OG",
        "outputId": "e32a1a99-5302-4296-dcfb-b04409ce8178"
      },
      "outputs": [],
      "source": [
        "epochs = 500\n",
        "batch_size = 64#32\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozEdPeKvv7OH"
      },
      "source": [
        "## Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCT0Nn8Dv7OH"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)\n",
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = y_pred.round(2)\n",
        "\n",
        "y_test[205]\n",
        "print(len(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xxx = np.zeros(len(y_pred))\n",
        "\n",
        "arr = np.arange(num_classes)\n",
        "for i in range(0,len(xxx)):\n",
        "    xxx[i]= np.dot(arr,y_pred[i])\n",
        "\n",
        "print(np.sum(abs(xxx-y_test))) #it should return 0 if all the classification are correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP3bLTr_v7OI"
      },
      "outputs": [],
      "source": [
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "timeseries_classification_from_scratch",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
